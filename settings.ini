[file]
# csv file with head name ["sentence", "label"]
train_file = data/twitter_train.csv
test_file = data/twitter_test.csv
# csv file with head name ["sentence"]
predict_file = data/twitter_test.csv

embedding_file = resources/GoogleNews-vectors-negative300.bin
# embedding_file = resources/glove.6B.100d.txt

# dir where trained model will be saved
model_dir = models
# dir where vocab dict file and predict result file will be saved
result_dir = results

[model]
# text label size
num_classes = 3

[train]
epochs = 20
batch_size = 32
# use gpu or not
cuda = false

[process]
# Sequences longer than this will be filtered out, and shorter than this will be padded with PAD.
max_sentence_len = 35
# Vocab num less than this will be dropped out.
min_word_count = 1